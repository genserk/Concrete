{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete = pd.read_csv('data/concrete.csv')\n",
    "co2 = pd.read_csv('data/co2.csv')\n",
    "\n",
    "concrete['exp_age'] = np.exp(-.038 * concrete['age'])\n",
    "\n",
    "concrete['co2_lower'] = sum([concrete[col] * co2.loc[co2.ingredient == col, 'lower_bound'].values[0] for col in concrete.columns[:7]])\n",
    "concrete['co2_upper'] = sum([concrete[col] * co2.loc[co2.ingredient == col, 'upper_bound'].values[0] for col in concrete.columns[:7]])\n",
    "\n",
    "concrete = concrete[concrete['age'] < 120]\n",
    "\n",
    "concrete_train, concrete_test = train_test_split(concrete,\n",
    "                                                 shuffle=True,\n",
    "                                                 random_state=487)\n",
    "\n",
    "features = ['cement', 'slag', 'water', 'superplastic', 'coarseagg', 'fineagg', 'exp_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was modified from stackexchange user hughdbrown \n",
    "# at this link, \n",
    "# https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "\n",
    "# This returns the power set of a set minus the empty set\n",
    "def powerset(s):\n",
    "    power_set = []\n",
    "    x = len(s)\n",
    "    for i in range(1 << x):\n",
    "        power_set.append([s[j] for j in range(x) if (i & (1 << j))])\n",
    "        \n",
    "    return power_set[1:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for future use, this function gets mean squared error without constantly copy-pasting\n",
    "\n",
    "def get_slr_mses(data, features_list, y, n_splits=5, rs=97):\n",
    "    # data is the dataframe\n",
    "    # features_list is a list of all lists of features we wish to compare\n",
    "    # eg [[], ['feature1'], ['feature1', 'feature2, 'feature5']]\n",
    "    # if one list is [], then we make a baseline prediction\n",
    "    # y is the y feature we are predicting\n",
    "    # k is the number of cross-validation splits\n",
    "    # rs is the random_state for kfold\n",
    "    kfold = KFold(n_splits,\n",
    "              shuffle=True,\n",
    "              random_state=rs)\n",
    "    mses=np.zeros((n_splits, len(features_list)))\n",
    "\n",
    "    i = 0\n",
    "    # cross-validation\n",
    "    for train_index, test_index in kfold.split(data):\n",
    "        data_tt = data.iloc[train_index]\n",
    "        data_ho = data.iloc[test_index]\n",
    "\n",
    "        j = 0\n",
    "        for features in features_list:\n",
    "            if features == []:\n",
    "                # baseline prediction\n",
    "                pred = data_tt[y].values.mean() * np.ones(len(data_ho))\n",
    "            else:\n",
    "                reg = LinearRegression(copy_X=True)\n",
    "                reg.fit(data_tt[features], data_tt[y])\n",
    "                pred = reg.predict(data_ho[features])\n",
    "            \n",
    "            mses[i, j] = mean_squared_error(y_true=data_ho[y],\n",
    "                                            y_pred=pred)\n",
    "            j += 1\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return np.mean(mses, axis=0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempt to improve the linear model a little by picking the exponential factor with all features in consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n",
    "       'fineagg', 'exp_age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 57.70330104192804\n",
      "0.02 51.14081744397187\n",
      "0.03 47.781495017706575\n",
      "0.04 47.158922486225194\n",
      "0.05 48.18470524244171\n",
      "0.060000000000000005 50.0325012084111\n",
      "0.06999999999999999 52.172666716534025\n",
      "0.08 54.32070776381877\n",
      "0.09 56.351408732591054\n"
     ]
    }
   ],
   "source": [
    "features2 = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n",
    "       'fineagg', 'exp_age']\n",
    "\n",
    "for factor in np.arange(.01, .1, .01):\n",
    "    concrete_train['exp_age'] = np.exp(-factor * concrete_train['age'])\n",
    "    print(factor, np.min(get_slr_mses(concrete_train, powerset(features2), 'strength')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038000000000000006 47.11634037831618\n"
     ]
    }
   ],
   "source": [
    "best_mses = []\n",
    "\n",
    "for factor in np.arange(.03, .05, .001):\n",
    "    concrete_train['exp_age'] = np.exp(-factor * concrete_train['age'])\n",
    "    best_mses.append(np.min(get_slr_mses(concrete_train, powerset(features2), 'strength')))\n",
    "\n",
    "\n",
    "print(np.arange(.03, .05, .001)[np.argmin(best_mses)], np.min(best_mses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_train['exp_age'] = np.exp(-.038 * concrete_train['age'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 45, 'max_features': None, 'n_estimators': 200, 'n_jobs': 1}\n",
      "29.750892287963836\n"
     ]
    }
   ],
   "source": [
    "max_depths = [45]\n",
    "features = ['cement', 'slag', 'water', 'superplastic', 'coarseagg', 'fineagg', 'exp_age']\n",
    "n_trees = [100, 200, 300]\n",
    "n_jobs = [1]\n",
    "max_features = [None]\n",
    "\n",
    "\n",
    "grid_cv = GridSearchCV(RandomForestRegressor(), \n",
    "                          param_grid = {'max_depth':max_depths, \n",
    "                                        'n_estimators':n_trees,\n",
    "                                        'n_jobs': n_jobs,\n",
    "                                        'max_features': max_features}, \n",
    "                          scoring = 'neg_mean_squared_error', \n",
    "                          cv = 5) \n",
    "\n",
    "## you fit it just like a model\n",
    "grid_cv.fit(concrete_train[features], concrete_train['strength'])\n",
    "\n",
    "print(grid_cv.best_params_)\n",
    "print(-grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 46, 'n_estimators': 100}\n",
      "29.206099330106305\n"
     ]
    }
   ],
   "source": [
    "max_depths = range(40, 51, 1)\n",
    "features = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'fineagg', 'age', 'exp_age']\n",
    "n_trees = [100, 500]\n",
    "\n",
    "grid_cv = GridSearchCV(RandomForestRegressor(), \n",
    "                          param_grid = {'max_depth':max_depths, \n",
    "                                        'n_estimators':n_trees}, \n",
    "                          scoring = 'neg_mean_squared_error',\n",
    "                          cv = 5) \n",
    "\n",
    "## you fit it just like a model\n",
    "grid_cv.fit(concrete_train[features], concrete_train['strength'])\n",
    "\n",
    "print(grid_cv.best_params_)\n",
    "print(-grid_cv.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4, 'n_estimators': 600, 'tree_method': 'exact'}\n",
      "21.418870786272343\n"
     ]
    }
   ],
   "source": [
    "max_depths = range(1, 6)\n",
    "features = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'fineagg', 'age', 'exp_age']\n",
    "n_trees = [400, 500, 600]\n",
    "# boosters = ['gbtree', 'gblinear', 'dart']\n",
    "tree_methods = ['exact', 'approx', 'hist']\n",
    "\n",
    "grid_cv = GridSearchCV(XGBRegressor(learning_rate=.1,\n",
    "                                ), \n",
    "                          param_grid = {'tree_method': tree_methods,\n",
    "                                        'max_depth':max_depths, \n",
    "                                        'n_estimators':n_trees,\n",
    "                                    #     'booster': boosters\n",
    "                                        }, \n",
    "                          scoring = 'neg_mean_squared_error', \n",
    "                          cv = 5)\n",
    "\n",
    "## you fit it just like a model\n",
    "grid_cv.fit(concrete_train[features], concrete_train['strength']\n",
    "                   )\n",
    "print(grid_cv.best_params_)\n",
    "print(-grid_cv.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram-based Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 5, 'max_iter': 800}\n",
      "21.90660451714236\n"
     ]
    }
   ],
   "source": [
    "max_depths = [2, 5, 10]\n",
    "max_iters = [600, 700, 800]\n",
    "features = ['cement', 'slag', 'water', 'superplastic', 'coarseagg', 'fineagg', 'age']\n",
    "learning_rates = [.1]\n",
    "\n",
    "grid_cv = GridSearchCV(HistGradientBoostingRegressor(loss='squared_error',\n",
    "                                                     learning_rate=.1,\n",
    "                                ), \n",
    "                          param_grid = {'max_depth':max_depths,\n",
    "                                        'max_iter': max_iters,\n",
    "                                        'learning_rate': learning_rates,\n",
    "                                    #     'booster': boosters\n",
    "                                        }, \n",
    "                          scoring = 'neg_mean_squared_error', \n",
    "                          cv = 5) \n",
    "\n",
    "## you fit it just like a model\n",
    "grid_cv.fit(concrete_train[features], concrete_train['strength']\n",
    "                   )\n",
    "print(grid_cv.best_params_)\n",
    "print(-grid_cv.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best I have gotten it, after much tinkering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = scale.fit_transform(concrete_train[features]), concrete_train['strength']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_tt, concrete_val = train_test_split(concrete_train,\n",
    "                                             shuffle=True,\n",
    "                                             random_state=453)\n",
    "\n",
    "X_tt, y_tt = scale.fit_transform(concrete_tt[features]), concrete_tt['strength']\n",
    "X_val, y_val = scale.fit_transform(concrete_val[features]), concrete_val['strength']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (200, 200), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "29.370374849389577\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes = [(200, 200), (250, 250)]\n",
    "learning_rates = ['constant', 'invscaling', 'adaptive']\n",
    "solvers = ['sgd']\n",
    "activations = ['relu']\n",
    "alphas = [.0001]\n",
    "\n",
    "grid_cv = GridSearchCV(MLPRegressor(max_iter=10000), \n",
    "                          param_grid = {'hidden_layer_sizes': hidden_layer_sizes,\n",
    "                                        'learning_rate': learning_rates,\n",
    "                                        'solver': solvers,\n",
    "                                        'activation': activations,\n",
    "                                        'alpha': alphas\n",
    "                                        }, \n",
    "                          scoring = 'neg_mean_squared_error', \n",
    "                          cv = 5) \n",
    "\n",
    "\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print(grid_cv.best_params_)\n",
    "print(-grid_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the following\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.Sequential()\n",
    "model1.add(layers.Dense(200, activation='relu', input_shape=(X_tt.shape[1],)))\n",
    "model1.add(layers.Dense(100, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "model1.compile(optimizer = 'rmsprop',\n",
    "                 loss = 'mean_squared_error',\n",
    "                 metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.37841033935547"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = keras.Sequential()\n",
    "model1.add(layers.Dense(100, activation='relu', input_shape=(X_tt.shape[1],)))\n",
    "model1.add(layers.Dense(100, activation='relu'))\n",
    "model1.add(layers.Dense(100, activation='relu'))\n",
    "model1.add(layers.Dense(100, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "model1.compile(optimizer = 'rmsprop',\n",
    "                 loss = 'mean_squared_error',\n",
    "                 metrics = ['mse'])\n",
    "n_epochs = 500\n",
    "history1 = model1.fit(X_tt,\n",
    "                       y_tt,\n",
    "                       epochs = n_epochs,\n",
    "                       batch_size = 50,\n",
    "                       verbose=0,\n",
    "                       validation_data = (X_val, \n",
    "                                          y_val))\n",
    "\n",
    "np.min(history1.history['val_mse'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
